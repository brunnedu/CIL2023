{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import Callable, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, jaccard_score, balanced_accuracy_score, mean_squared_error\n",
    "\n",
    "from src.mask_to_submission_old import mask_to_submission_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_dir = \"./data/training/images\"  # satellite images\n",
    "gt_dir = \"./data/training/groundtruth\"  # groundtruth for satellite images\n",
    "\n",
    "experiment_name = \"test_run_2023-06-23_15-34-04\"\n",
    "threshold = 0.5  # make sure to use the same threshold as in submission call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pred_dir = f\"out/{experiment_name}/run\"  # model output\n",
    "sub_dir = f\"out/{experiment_name}/submission{int(threshold*100)}\"  # submission output\n",
    "\n",
    "# generate ground truth submission directory\n",
    "gt_sub_dir = f\"out/{experiment_name}/submission{int(threshold*100)}_gt\"\n",
    "os.makedirs(gt_sub_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(gt_dir):\n",
    "    # ignore generator output\n",
    "    _ = list(mask_to_submission_strings(\n",
    "        image_filename=os.path.join(gt_dir, filename),\n",
    "        mask_dir=gt_sub_dir,\n",
    "        foreground_threshold=threshold\n",
    "    ))\n",
    "\n",
    "# generate ground truth submission directory\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "# generate prediction submission\n",
    "for filename in os.listdir(pred_dir):\n",
    "    # ignore generator output\n",
    "    _ = list(mask_to_submission_strings(\n",
    "        image_filename=os.path.join(pred_dir, filename),\n",
    "        mask_dir=sub_dir,\n",
    "        foreground_threshold=threshold\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# assuming images are in the same order\n",
    "assert os.listdir(gt_dir) == os.listdir(pred_dir), \\\n",
    "    \"prediction and groundtruth directories have a different structure\"\n",
    "\n",
    "image_names = np.array(os.listdir(gt_dir))\n",
    "\n",
    "def name_from_index(i: int):\n",
    "    return image_names[i]\n",
    "\n",
    "def index_from_name(name: str):\n",
    "    return np.where(image_names == name)[0][0]\n",
    "\n",
    "images = np.array([np.array(Image.open(os.path.join(image_dir, fname))) for fname in image_names])\n",
    "gt_images = np.array([np.array(Image.open(os.path.join(gt_dir, fname))) for fname in image_names])\n",
    "pred_images = np.array([np.array(Image.open(os.path.join(pred_dir, fname))) for fname in image_names])\n",
    "sub_images = np.array([np.array(Image.open(os.path.join(sub_dir, f\"mask_{fname}\")), dtype=bool) for fname in image_names])\n",
    "gt_sub_images = np.array([np.array(Image.open(os.path.join(gt_sub_dir, f\"mask_{fname}\")), dtype=bool) for fname in image_names])\n",
    "\n",
    "gt_images_bool = (gt_images / 255) > threshold\n",
    "pred_images_bool = (pred_images / 255) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_metric(stat_func: Callable, y_true: np.array, y_pred: np.array, **kwargs):\n",
    "    res = np.zeros(gt_images.shape[0], dtype=float)\n",
    "    for i in range(gt_images.shape[0]):\n",
    "        score = stat_func(y_true=y_true[i].ravel(), y_pred=y_pred[i].ravel(), **kwargs)\n",
    "        res[i] = score\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# threshold prediction (values between 0 and 1, relevant for boxplot)\n",
    "threshold_metrics = {\n",
    "    \"Jaccard\": compute_metric(jaccard_score, gt_images_bool, pred_images_bool, zero_division=1),\n",
    "    \"F1\": compute_metric(f1_score, gt_images_bool, pred_images_bool, zero_division=1),\n",
    "    \"Bal. Acc.\": compute_metric(balanced_accuracy_score, gt_images_bool, pred_images_bool),\n",
    "}\n",
    "\n",
    "sub_metrics = {\n",
    "    \"Jaccard\": compute_metric(jaccard_score, gt_sub_images, sub_images, zero_division=1),\n",
    "    \"F1\": compute_metric(f1_score, gt_sub_images, sub_images, zero_division=1),\n",
    "    \"Bal. Acc.\": compute_metric(balanced_accuracy_score, gt_sub_images, sub_images),\n",
    "}\n",
    "\n",
    "# raw prediction\n",
    "raw_metrics = {\n",
    "    \"MSE\": compute_metric(mean_squared_error, gt_images / 255, pred_images / 255),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_statistics(items: List[Dict], should_plot=True):\n",
    "    \"\"\"\n",
    "    :param items: list of dictionaries\n",
    "    :param should_plot: If boxplot should be plotted (will always plot in Jupyter).\n",
    "    :return: List of dictionaries with name of metric, statistics, and values\n",
    "    \"\"\"\n",
    "\n",
    "    bp_data = plt.boxplot(list(items.values()), labels=list(items.keys()))\n",
    "    metric_stats = []\n",
    "    for i, (metric, values) in enumerate(items.items()):\n",
    "        stats = {\n",
    "            \"lower_whisker\": bp_data[\"whiskers\"][i * 2].get_ydata()[1],\n",
    "            \"lower_quartile\": bp_data[\"boxes\"][i].get_ydata()[1],\n",
    "            \"median\": bp_data[\"medians\"][i].get_ydata()[1],\n",
    "            \"upper_quartile\": bp_data[\"boxes\"][i].get_ydata()[2],\n",
    "            \"upper_whisker\": bp_data[\"whiskers\"][(i * 2) + 1].get_ydata()[1],\n",
    "            \"mean\": values.mean(),\n",
    "            \"min\": values.min(),\n",
    "            \"max\": values.max(),\n",
    "            \"best\": name_from_index(np.argmax(values)),\n",
    "            \"worst\": name_from_index(np.argmin(values))\n",
    "        }\n",
    "        stats[\"bad_outlier_indices\"] = np.where(values < stats[\"lower_whisker\"])[0]\n",
    "        stats[\"bad_outlier_images\"] = name_from_index(stats[\"bad_outlier_indices\"])\n",
    "\n",
    "        stats[\"good_outlier_indices\"] = np.where(values > stats[\"upper_whisker\"])[0]\n",
    "        stats[\"good_outlier_images\"] = name_from_index(stats[\"good_outlier_indices\"])\n",
    "\n",
    "        metric_stats.append({\n",
    "            \"metric\": metric,\n",
    "            \"stats\": stats,\n",
    "            \"values\": values\n",
    "        })\n",
    "\n",
    "    if should_plot:\n",
    "        for i, metric_stat in enumerate(metric_stats):\n",
    "            outliers = np.concatenate(\n",
    "                (metric_stat[\"stats\"][\"bad_outlier_indices\"], metric_stat[\"stats\"][\"good_outlier_indices\"])\n",
    "            )\n",
    "            for outlier_index in outliers:\n",
    "                # 0.08 cause of overlaps\n",
    "                plt.annotate(name_from_index(outlier_index), xy=(1+i+0.08, metric_stat[\"values\"][outlier_index]))\n",
    "        plt.plot()\n",
    "\n",
    "    return metric_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# current run has two outliers at 0 for both Jaccard and F1\n",
    "# if annotations overlap, outliers can also be seen in the printed statistics\n",
    "threshold_stats = compute_statistics(items=threshold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sub_stats = compute_statistics(items=sub_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_stats = compute_statistics(items=raw_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_stats(metric_stats):\n",
    "    for metric_stat in metric_stats:\n",
    "        print(metric_stat[\"metric\"])\n",
    "        stat_names = metric_stat[\"stats\"].keys()\n",
    "        longest_name = len(max(stat_names, key=len))\n",
    "        print(\"-\" * longest_name)\n",
    "        for stat_name, stat_value in metric_stat[\"stats\"].items():\n",
    "            formatted_value = stat_value\n",
    "            if isinstance(stat_value, float):\n",
    "                formatted_value = f\"{stat_value:.4f}\"\n",
    "            print(f\"{stat_name.ljust(longest_name)} {formatted_value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"** Submission Statistics **\")\n",
    "print_stats(sub_stats)\n",
    "# print_stats(threshold_stats)\n",
    "# print_stats(raw_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def overlay_mask(image: np.array, mask: np.array, clear_gb_channels=True):\n",
    "    # add mask to image\n",
    "    image = image.copy().astype(np.float32)\n",
    "    image[...,0] = np.minimum(255, image[...,0] + mask)  # set red channel to 255\n",
    "\n",
    "    if clear_gb_channels:\n",
    "        image[...,1][mask == 255] = 0  # set green channel to zero\n",
    "        image[...,2][mask == 255] = 0  # set blue channel to zero\n",
    "\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "def plot_prediction(i):\n",
    "    pred_bool = ((pred_images[i] / 255) > threshold) * 255\n",
    "\n",
    "    img = np.asarray(Image.fromarray(images[i]).convert(\"RGB\"))  # remove transparency\n",
    "    img_gt_overlay = overlay_mask(img, gt_images[i])\n",
    "    img_pred_overlay = overlay_mask(img, pred_images[i])\n",
    "    img_pred_bool_overlay = overlay_mask(img, pred_bool)\n",
    "\n",
    "    fig, axarr = plt.subplot_mosaic(\"ABC;DEF;GHI\", width_ratios=[1,1,0.6])\n",
    "    fig.set_size_inches(7, 8)\n",
    "\n",
    "    # Ground Truth\n",
    "    axarr[\"A\"].imshow(Image.fromarray(gt_images[i]), cmap=\"gray\")\n",
    "    axarr[\"B\"].imshow(img_gt_overlay)\n",
    "    axarr[\"A\"].set_title(\"Ground Truth\")\n",
    "\n",
    "    # Raw Prediction\n",
    "    axarr[\"D\"].imshow(Image.fromarray(pred_images[i]), cmap=\"gray\")\n",
    "    axarr[\"E\"].imshow(img_pred_overlay)\n",
    "    axarr[\"D\"].set_title(\"Raw Prediction\")\n",
    "\n",
    "    # Threshold Prediction\n",
    "    axarr[\"G\"].imshow(pred_bool, cmap=\"gray\")\n",
    "    axarr[\"H\"].imshow(img_pred_bool_overlay)\n",
    "    axarr[\"G\"].set_title(f\"Threshold Prediction ({threshold})\")\n",
    "\n",
    "    [axis[1].set_axis_off() for axis in list(axarr.items())]\n",
    "    table_options = {\"loc\": \"center\", \"colWidths\": [0.4, 0.6]}\n",
    "\n",
    "    # Ground Truth Table\n",
    "    table_c = axarr[\"C\"].table([[\"Image\", name_from_index(i)], [\"Index\", i]], **table_options)\n",
    "    table_c.scale(1.5, 1.5)\n",
    "\n",
    "    # Raw Prediction Table\n",
    "    table_f = axarr[\"F\"].table([\n",
    "        [m[\"metric\"], f\"{m['values'][i]:.4f}\"] for m in raw_stats\n",
    "    ], **table_options)\n",
    "    table_f.scale(1.5, 1.5)\n",
    "\n",
    "    # Threshold Prediction Table\n",
    "    table_i = axarr[\"I\"].table([\n",
    "        [m[\"metric\"], f\"{m['values'][i]:.4f}\"] for m in threshold_stats\n",
    "    ], **table_options)\n",
    "    table_i.scale(1.5, 1.5)\n",
    "\n",
    "    fig.suptitle(\"Full Predictions\", weight=\"bold\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "def plot_submission(i):\n",
    "    img = Image.fromarray(images[i]).convert(\"RGB\")  # remove transparency\n",
    "    img_gt_sub_overlay = overlay_mask(np.asarray(img), gt_sub_images[i] * 255)\n",
    "    img_sub_overlay = overlay_mask(np.asarray(img), sub_images[i] * 255)\n",
    "\n",
    "    fig, axarr = plt.subplot_mosaic(\"ABC;DEF\", width_ratios=[1,1,0.6])\n",
    "    fig.set_size_inches(7, 5)\n",
    "    [axis[1].set_axis_off() for axis in list(axarr.items())]\n",
    "\n",
    "    # Submission Ground Truth\n",
    "    axarr[\"A\"].imshow(Image.fromarray(gt_sub_images[i]))\n",
    "    axarr[\"B\"].imshow(img_gt_sub_overlay)\n",
    "    axarr[\"A\"].set_title(f\"Ground Truth ({threshold})\")\n",
    "\n",
    "    # Submission Prediction\n",
    "    axarr[\"D\"].imshow(Image.fromarray(sub_images[i]))\n",
    "    axarr[\"E\"].imshow(img_sub_overlay)\n",
    "    axarr[\"D\"].set_title(f\"Prediction ({threshold})\")\n",
    "\n",
    "    table_options = {\"loc\": \"center\", \"colWidths\": [0.4, 0.6]}\n",
    "    # Ground Truth Table\n",
    "    table_c = axarr[\"C\"].table([[\"Image\", name_from_index(i)], [\"Index\", i]], **table_options)\n",
    "    table_c.scale(1.5, 1.5)\n",
    "\n",
    "    # Submission Prediction Table\n",
    "    table_l = axarr[\"F\"].table([\n",
    "        [m[\"metric\"], f\"{m['values'][i]:.4f}\"] for m in sub_stats\n",
    "    ], **table_options)\n",
    "    table_l.scale(1.5, 1.5)\n",
    "\n",
    "    fig.suptitle(\"Submission\", weight=\"bold\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "n_best_indices = np.argsort(sub_stats[0][\"values\"])[-n:]\n",
    "\n",
    "for i in n_best_indices:\n",
    "    plot_prediction(i=i)\n",
    "    plot_submission(i=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "n_worst_indices = np.argsort(sub_stats[0][\"values\"])[:n]\n",
    "for i in n_worst_indices:\n",
    "    plot_prediction(i=i)\n",
    "    plot_submission(i=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
