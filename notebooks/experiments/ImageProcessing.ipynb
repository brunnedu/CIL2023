{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156d64d-e923-4ac3-b984-90619076936a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c19e00-8f52-4734-9c62-b15a3f91b736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ff086-e873-4446-b94b-75499eb254d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fd350-b2c0-4c97-87fc-855a84f0a3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.dataset import SatelliteDataset\n",
    "from src.models.unet.unet import UNet\n",
    "from src.models.unet.backbones import Resnet18Backbone\n",
    "from src.models.unet.blocks import UpBlock\n",
    "\n",
    "from src.utils import display_image, display_sample, fix_all_seeds, create_logger, load_checkpoint, save_model, save_checkpoint, save_plotting_data\n",
    "\n",
    "\n",
    "from src.transforms import NORMALIZATION_PARAMS_2022, NORMALIZATION_PARAMS_EQUALIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fa2c5-1e73-4c2b-a723-a68a98192fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f3768-f866-4301-a4a3-626f29df7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SatelliteDataset(data_dir=\"data/training\", hist_equalization=False)\n",
    "\n",
    "ds_add = SatelliteDataset(data_dir=\"data/training\", add_data_dir=\"data/data_2022\", hist_equalization=False)\n",
    "\n",
    "print(len(ds))\n",
    "print(len(ds_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05910ed-e27f-4283-91ae-607ef15f93a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img, mask = ds[3]\n",
    "display_image(img, normalization_params=NORMALIZATION_PARAMS_EQUALIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149111d-5514-42d1-9942-4a3562b4f782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_image(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe67944-7e76-4a2e-81a9-3168540571ff",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49a35a-57e7-432b-a371-6d10217b09ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import equalize\n",
    "from src.transforms import NORMALIZATION_PARAMS_2022\n",
    "import torchvision\n",
    "from torchvision.io import ImageReadMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883c129-c112-41bc-9294-e4b2bffe1768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = read_image(ds.img_paths[4], mode=ImageReadMode.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f930a-5af3-4fea-9c68-379cebb23315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "NORMALIZATION_PARAMS_2022 = {\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "}\n",
    "\n",
    "NORMALIZATION_TRANSFORM = Normalize(**NORMALIZATION_PARAMS_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b7922-ef48-42d6-ac6b-35c1bc24d080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f1124-f6d2-420f-8e25-deacf9ea9f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use cv2 to load image as uint8 numpy array (as suggested by albumentations)\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(ds.img_paths[3])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "mask = cv2.imread(ds.mask_paths[3])\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "transformed = transform(image=img, mask=mask)\n",
    "img, mask = torch.from_numpy(transformed['image']).permute(2, 0, 1), torch.from_numpy(transformed['mask']).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118d742-4d0f-49df-82b1-a5fee6295ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use torch tensor images from the dataset\n",
    "img, mask = ds[3]\n",
    "\n",
    "transformed = transform(image=img.permute(1, 2, 0).numpy(), mask=mask.permute(1, 2, 0).numpy())\n",
    "img, mask = torch.from_numpy(transformed['image']).permute(2, 0, 1), torch.from_numpy(transformed['mask']).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a091d-eee9-4b52-9517-13f98d9c18bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8f2e0-5ab6-4b53-84ac-ac0c208b01e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_image(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e38ac8-574c-43ec-bec7-de1bdc2f470d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for X, y in dl_train:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    # print(X)\n",
    "    # print(y)\n",
    "    \n",
    "    out = model(X)\n",
    "    print(out.shape)\n",
    "    print(out.min())\n",
    "    print(out.max())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3012a05-c0af-4ec2-b3f7-c154108c3694",
   "metadata": {},
   "source": [
    "### Histogram Equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206a3e3-e955-4877-9206-43feb41fe6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 12\n",
    "display_image(torch.stack([read_image(f, mode=ImageReadMode.RGB) for f in ds.img_paths[:N]]), nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea025157-5a2a-42da-bf7a-4d7c96ff3e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_image(torch.stack([equalize(read_image(f, mode=ImageReadMode.RGB)) for f in ds.img_paths[:N]]), nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873aec67-6df3-4bdb-be4d-e4801febe1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_eq_sample(idx):\n",
    "    display_image(torch.stack([read_image(ds_add.img_paths[idx], mode=ImageReadMode.RGB), equalize(read_image(ds_add.img_paths[idx], mode=ImageReadMode.RGB))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ca717-0fe2-4be8-8cf2-8d4700a01bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_eq_sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb64a6-cb2a-4c1c-b3a4-930a4682763a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_eq_sample(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f701e-87a6-4e0c-8bfc-97e055e2d34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_eq_sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72c507-ef1d-43c0-ae93-427018ba0154",
   "metadata": {},
   "source": [
    "### Calculate Mean and Standard Deviation of Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be545e74-efad-46fd-8b92-fc18777e9b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_t = Normalize(mean=(0.4931, 0.4934, 0.4928), std=(0.2903, 0.2905, 0.2906))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97af9f-53c7-44bd-b127-1ff53f721eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for img_path in ds.img_paths:\n",
    "    res = equalize(read_image(img_path, mode=ImageReadMode.RGB))/255\n",
    "    #res = norm_t(res)\n",
    "    imgs.append(res)\n",
    "imgs = torch.stack(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d5011-96a2-4aac-b907-5184d59c9cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = torch.mean(imgs, dim=(0, 2, 3))\n",
    "std = torch.std(imgs, dim=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e8bbe-2712-45b3-a9bf-ccfb399294e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac8c0a8-46bb-4760-8906-4e03287aa0eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8440aaa-e17c-45c1-9675-2cea2deda95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_accuracy_fn(y_hat, y):\n",
    "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
    "    h_patches = y.shape[-2] // PATCH_SIZE\n",
    "    w_patches = y.shape[-1] // PATCH_SIZE\n",
    "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    return (patches == patches_hat).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e03d5-5644-49bd-9635-0724ca54229c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.ones(1, 1, 20, 20)\n",
    "b = torch.zeros(1, 1, 20, 20)\n",
    "b[0, 0, :10, :10] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696d4ed-447c-4f0b-8fea-7e8948771c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def binary_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute binary F1 score between y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "    - y_true: true labels (torch.Tensor)\n",
    "    - y_pred: predicted labels (torch.Tensor)\n",
    "\n",
    "    Returns:\n",
    "    - binary F1 score (torch.Tensor)\n",
    "    \"\"\"\n",
    "    # Convert y_pred to binary values\n",
    "    y_pred = torch.round(y_pred)\n",
    "\n",
    "    # Compute true positives, false positives, and false negatives\n",
    "    tp = torch.sum((y_true == 1) & (y_pred == 1)).float()\n",
    "    fp = torch.sum((y_true == 0) & (y_pred == 1)).float()\n",
    "    fn = torch.sum((y_true == 1) & (y_pred == 0)).float()\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / (tp + fn + 1e-10)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "class PatchF1ScoreNew(PatchAccuracy):\n",
    "    \"\"\"\n",
    "    Evaluation metric used this year.\n",
    "    1. Splits the prediction and target into patches of size patch_size.\n",
    "    2. Binarizes every patch by comparing the mean of the patch activations to the cutoff value.\n",
    "    3. Computes the F1-Score over the binarized patches.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size: int = 16, cutoff: float = 0.25, eps: float = 1e-10):\n",
    "        super(PatchF1ScoreNew, self).__init__(patch_size=patch_size, cutoff=cutoff)\n",
    "        \n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "\n",
    "        patches_hat, patches = self.binarize_patches(y_hat, y)\n",
    "        \n",
    "        # Compute true positives, false positives, and false negatives\n",
    "        tp = torch.sum((patches_hat == 1) & (patches == 1)).float()\n",
    "        fp = torch.sum((patches_hat == 1) & (patches == 0)).float()\n",
    "        fn = torch.sum((patches_hat == 0) & (patches == 1)).float()\n",
    "\n",
    "        # Compute precision, recall, and F1 score\n",
    "        precision = tp / (tp + fp + self.eps)\n",
    "        recall = tp / (tp + fn + self.eps)\n",
    "        f1_score = 2 * precision * recall / (precision + recall + self.eps)\n",
    "\n",
    "        return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda70ce-50ab-4543-91cb-9954731ef8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.metrics import PatchAccuracy, PatchF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204702e-49d0-45ee-b5f1-3054f2fc5319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pa = PatchAccuracy(patch_size=2, cutoff=0.25)\n",
    "pf = PatchF1Score(patch_size=2, cutoff=0.25)\n",
    "pfn = PatchF1ScoreNew(patch_size=2, cutoff=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548a97c-d7f4-4051-88cd-c9d86edb56a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pa(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a6671-d974-4756-a2b8-17eda2764fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pf(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713bd85-319b-4ad7-afb1-86c029402d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pfn(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7fa40-f472-4f34-82eb-0cd519049eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
